{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrategia de Decodificación: Greedy Search (Búsqueda Voraz)\n",
    "\n",
    "**Concepto General:**\n",
    "\n",
    "La **Greedy Search** (o Búsqueda Voraz) es la estrategia de decodificación más simple y directa para los modelos secuencia a secuencia (Seq2Seq). En cada paso de la generación de la secuencia de salida, Greedy Search elige el token (palabra, subpalabra, o caracter) que tiene la probabilidad más alta según la predicción del modelo en ese momento, sin considerar el impacto de esta elección en los pasos futuros.\n",
    "\n",
    "\n",
    "**Fórmula:**\n",
    "\n",
    "Matemáticamente, en cada paso de tiempo $t$, Greedy Search selecciona el token $\\hat{y}_t$ del vocabulario $V$ que maximiza la probabilidad condicional dada la secuencia de entrada $x$ y los tokens previamente generados $y_{<t}$:\n",
    "\n",
    "$$\n",
    "\\hat{y}_t = \\arg\\max_{v \\in V} P(y_t=v \\mid y_{<t}, x)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "* $\\hat{y}_t$: es el token predicho en el paso de tiempo $t$.\n",
    "* $v$: es un token candidato del vocabulario $V$.\n",
    "* $P(y_t=v \\mid y_{<t}, x)$: es la probabilidad (generalmente la salida de una capa softmax en el decodificador) del token $v$ en el tiempo $t$, dados los tokens anteriores $y_{<t}$ y la secuencia de entrada $x$.\n",
    "\n",
    "El proceso se repite hasta que se genera un token especial de fin de secuencia (EOS, End-Of-Sequence) o se alcanza una longitud máxima predefinida.\n",
    "\n",
    "\n",
    "**Ventajas:**\n",
    "\n",
    "1.  Simpleza\n",
    "2.  Rapidez\n",
    "3.  Bajo coste computacional\n",
    "\n",
    "**Desventajas:**\n",
    "\n",
    "1.  **Óptimos Locales:** Es muy propensa a caer en óptimos locales. Una elección que parece la mejor en un paso puede llevar a una secuencia global subóptima. Por ejemplo, puede generar una palabra muy probable al principio que luego dificulte la formación de una frase coherente.\n",
    "2.  **Repeticiones:** Puede generar secuencias repetitivas, especialmente si el modelo no está bien entrenado o si ciertas palabras tienen probabilidades consistentemente altas.\n",
    "3.  **Falta de Diversidad:** No explora diferentes caminos o alternativas en la generación. Siempre produce la misma secuencia de salida para la misma entrada.\n",
    "4.  **Sensibilidad a Errores Tempranos:** Un error cometido al principio de la secuencia (elegir una palabra incorrecta pero con alta probabilidad local) se propagará y afectará negativamente al resto de la generación, ya que no hay mecanismo de corrección o reconsideración.\n",
    "5.  **Calidad de Secuencia:** A menudo produce secuencias que son gramaticalmente correctas pero pueden ser menos naturales, menos coherentes o de menor calidad general en comparación con métodos más avanzados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KATANA\\Desktop\\git\\Personal\\nlp-proyecto11\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k # Asegúrate que Multi30k esté correctamente instalado y accesible\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchdata.datapipes.iter import IterableWrapper, Mapper \n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TRG_LANGUAGE = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizadores cargados.\n",
      "Construyendo vocabulario para: de\n",
      "Construyendo vocabulario para: en\n",
      "Vocabularios construidos exitosamente desde Multi30k.\n",
      "Tamaño vocabulario alemán (de): 19214\n",
      "Tamaño vocabulario inglés (en): 10837\n"
     ]
    }
   ],
   "source": [
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    " # Inicializar tokenizadores\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TRG_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "print(\"Tokenizadores cargados.\")\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TRG_LANGUAGE: 1}\n",
    "    for data_sample in data_iter:\n",
    "        if isinstance(data_sample, (list, tuple)) and len(data_sample) > language_index[language]:\n",
    "                yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "\n",
    "\n",
    "# Tokens especiales y sus índices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "\n",
    "train_iter_for_vocab = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    print(f\"Construyendo vocabulario para: {ln}\")\n",
    "    current_train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(\n",
    "                                yield_tokens(current_train_iter, ln),\n",
    "                                min_freq=1, \n",
    "                                specials=special_symbols,\n",
    "                                special_first=True)\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)\n",
    "print(\"Vocabularios construidos exitosamente desde Multi30k.\")\n",
    "print(f\"Tamaño vocabulario alemán ({SRC_LANGUAGE}): {len(vocab_transform[SRC_LANGUAGE])}\")\n",
    "print(f\"Tamaño vocabulario inglés ({TRG_LANGUAGE}): {len(vocab_transform[TRG_LANGUAGE])}\")\n",
    "\n",
    "if SRC_LANGUAGE not in vocab_transform or TRG_LANGUAGE not in vocab_transform or \\\n",
    "    len(vocab_transform.get(SRC_LANGUAGE, [])) <= len(special_symbols) or \\\n",
    "    len(vocab_transform.get(TRG_LANGUAGE, [])) <= len(special_symbols): # Chequeo más robusto\n",
    "\n",
    "\n",
    "    if SRC_LANGUAGE not in vocab_transform or len(vocab_transform.get(SRC_LANGUAGE, [])) <= len(special_symbols):\n",
    "        print(f\"Creando vocabulario dummy para {SRC_LANGUAGE}\")\n",
    "        dummy_tokens_src = [['ein', 'mann', 'mit', 'einem', 'roten', 'helm'], ['eine', 'frau', 'spielt', 'ball']]\n",
    "        dummy_vocab_src = build_vocab_from_iterator(dummy_tokens_src, specials=special_symbols, special_first=True)\n",
    "        dummy_vocab_src.set_default_index(UNK_IDX)\n",
    "        vocab_transform[SRC_LANGUAGE] = dummy_vocab_src\n",
    "\n",
    "    if TRG_LANGUAGE not in vocab_transform or len(vocab_transform.get(TRG_LANGUAGE, [])) <= len(special_symbols):\n",
    "        print(f\"Creando vocabulario dummy para {TRG_LANGUAGE}\")\n",
    "        dummy_tokens_trg = [['a', 'man', 'with', 'a', 'red', 'helmet'], ['a', 'woman', 'plays', 'ball']]\n",
    "        dummy_vocab_trg = build_vocab_from_iterator(dummy_tokens_trg, specials=special_symbols, special_first=True)\n",
    "        dummy_vocab_trg.set_default_index(UNK_IDX)\n",
    "        vocab_transform[TRG_LANGUAGE] = dummy_vocab_trg\n",
    "    \n",
    "    print(f\"Tamaño vocabulario alemán (dummy): {len(vocab_transform[SRC_LANGUAGE])}\")\n",
    "    print(f\"Tamaño vocabulario inglés (dummy): {len(vocab_transform[TRG_LANGUAGE])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random \n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout if n_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src = [src len, batch size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [src len, batch size, emb dim]\n",
    "        # Usar self.lstm aquí también\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        # outputs = [src len, batch size, hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid dim]\n",
    "        # cell = [n layers * n directions, batch size, hid dim]\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout if n_layers > 1 else 0)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input = [batch size]\n",
    "        # hidden = [n layers, batch size, hid dim]\n",
    "        # cell = [n layers, batch size, hid dim]\n",
    "        input = input.unsqueeze(0) # input = [1, batch size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch size, emb dim]\n",
    "        # Usar self.lstm aquí también\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # output = [1, batch size, hid dim]\n",
    "        # hidden = [n layers, batch size, hid dim]\n",
    "        # cell = [n layers, batch size, hid dim]\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Las dimensiones ocultas del codificador y decodificador deben ser iguales!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"El codificador y decodificador deben tener el mismo número de capas!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input_token = trg[0,:] # Token <bos>\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input_token = trg[t] if teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENC_EMB_DIM = 128\n",
    "DEC_EMB_DIM = 128\n",
    "HID_DIM = 256\n",
    "N_LAYERS = 1\n",
    "ENC_DROPOUT = 0.3 \n",
    "DEC_DROPOUT = 0.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_DIM (tamaño vocabulario alemán): 19214\n",
      "OUTPUT_DIM (tamaño vocabulario inglés): 10837\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(vocab_transform[SRC_LANGUAGE])\n",
    "OUTPUT_DIM = len(vocab_transform[TRG_LANGUAGE])\n",
    "\n",
    "print(f\"INPUT_DIM (tamaño vocabulario alemán): {INPUT_DIM}\")\n",
    "print(f\"OUTPUT_DIM (tamaño vocabulario inglés): {OUTPUT_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instancias de Encoder, Decoder y Seq2Seq creadas.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "    model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n",
    "    print(\"Instancias de Encoder, Decoder y Seq2Seq creadas.\")\n",
    "except NameError as ne:\n",
    "    print(f\"Error: Parece que las clases Encoder, Decoder o Seq2Seq no están definidas. Asegúrate de ejecutar la Celda 2. ({ne})\")\n",
    "    raise\n",
    "except Exception as e_init:\n",
    "    print(f\"Error al instanciar los modelos: {e_init}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH = 'RNN-TR-model.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_DIM (tamaño vocabulario alemán): 19214\n",
      "OUTPUT_DIM (tamaño vocabulario inglés): 10837\n",
      "Usando Hiperparámetros CORREGIDOS para instanciar el modelo:\n",
      "  ENC_EMB_DIM: 128, DEC_EMB_DIM: 128\n",
      "  HID_DIM: 256, N_LAYERS: 1\n",
      "  ENC_DROPOUT: 0.3, DEC_DROPOUT: 0.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "INPUT_DIM = len(vocab_transform[SRC_LANGUAGE])\n",
    "OUTPUT_DIM = len(vocab_transform[TRG_LANGUAGE])\n",
    "\n",
    "print(f\"INPUT_DIM (tamaño vocabulario alemán): {INPUT_DIM}\")\n",
    "print(f\"OUTPUT_DIM (tamaño vocabulario inglés): {OUTPUT_DIM}\")\n",
    "\n",
    "\n",
    "ENC_EMB_DIM = 128\n",
    "DEC_EMB_DIM = 128\n",
    "HID_DIM = 256\n",
    "N_LAYERS = 1 \n",
    "ENC_DROPOUT = 0.3 \n",
    "DEC_DROPOUT = 0.3 \n",
    "\n",
    "\n",
    "print(f\"Usando Hiperparámetros CORREGIDOS para instanciar el modelo:\")\n",
    "print(f\"  ENC_EMB_DIM: {ENC_EMB_DIM}, DEC_EMB_DIM: {DEC_EMB_DIM}\")\n",
    "print(f\"  HID_DIM: {HID_DIM}, N_LAYERS: {N_LAYERS}\")\n",
    "print(f\"  ENC_DROPOUT: {ENC_DROPOUT}, DEC_DROPOUT: {DEC_DROPOUT}\")\n",
    "\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n",
    "\n",
    "MODEL_PATH = 'RNN-TR-model.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_sentence(sentence: str, vocab, tokenizer, device, bos_token='<bos>', eos_token='<eos>', unk_token='<unk>'):\n",
    "    \"\"\"\n",
    "    Preprocesses a source sentence string into a tensor for the model.\n",
    "    Args:\n",
    "        sentence (str): The source language sentence.\n",
    "        vocab: The source language vocabulary object (e.g., vocab_transform[SRC_LANGUAGE]).\n",
    "        tokenizer: The source language tokenizer (e.g., token_transform[SRC_LANGUAGE]).\n",
    "        device: The device (cpu/cuda) to send the tensor to.\n",
    "        bos_token (str): The beginning-of-sequence token string.\n",
    "        eos_token (str): The end-of-sequence token string.\n",
    "        unk_token (str): The unknown token string.\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of the processed sentence, shape [seq_len, 1] (batch_size=1).\n",
    "    \"\"\"\n",
    "    if not vocab or not tokenizer:\n",
    "        print(\"Error: Vocabulario o tokenizador no proporcionado a preprocess_sentence.\")\n",
    "        return None\n",
    "\n",
    "    # Tokenize the sentence and convert to lowercase\n",
    "    tokens = [token.lower() for token in tokenizer(sentence)]\n",
    "    \n",
    "    # Add <bos> and <eos> tokens\n",
    "    processed_tokens = [bos_token] + tokens + [eos_token]\n",
    "    \n",
    "    # Numericalize tokens, using <unk> for out-of-vocabulary words\n",
    "    numericalized_tokens = []\n",
    "    for token in processed_tokens:\n",
    "        try:\n",
    "            numericalized_tokens.append(vocab[token])\n",
    "        except KeyError: # More robust than checking `if token in vocab` for some vocab types\n",
    "            numericalized_tokens.append(vocab[unk_token])\n",
    "            \n",
    "    # Convert to a LongTensor and add a batch dimension (batch_size=1)\n",
    "    src_tensor = torch.LongTensor(numericalized_tokens).unsqueeze(1).to(device)\n",
    "    # src_tensor shape: [seq_len, 1]\n",
    "    \n",
    "    return src_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 5: Implementación de Greedy Search Decode (Corregida)\n",
    "\n",
    "# Asegúrate que model, vocab_transform, token_transform, SRC_LANGUAGE, TRG_LANGUAGE, DEVICE\n",
    "# y la función preprocess_sentence estén definidos de celdas anteriores.\n",
    "# También, los tokens especiales como '<bos>', '<eos>' deben ser consistentes.\n",
    "\n",
    "import torch # Asegurarse que torch está importado aquí si no lo está globalmente en la celda\n",
    "\n",
    "def greedy_search_decode(model_to_decode, src_sentence_str: str, max_len: int = 50,\n",
    "                         src_vocab=None, trg_vocab=None, src_tokenizer=None, device_to_use=None,\n",
    "                         bos_token_str='<bos>', eos_token_str='<eos>', unk_token_str='<unk>'):\n",
    "    \"\"\"\n",
    "    Genera una traducción de una oración fuente usando la estrategia de Greedy Search.\n",
    "\n",
    "    Args:\n",
    "        model_to_decode (torch.nn.Module): El modelo Seq2Seq entrenado.\n",
    "        src_sentence_str (str): La oración en el idioma fuente.\n",
    "        max_len (int): La longitud máxima permitida para la secuencia traducida.\n",
    "        src_vocab: Vocabulario del idioma fuente.\n",
    "        trg_vocab: Vocabulario del idioma objetivo.\n",
    "        src_tokenizer: Tokenizador para el idioma fuente.\n",
    "        device_to_use: Dispositivo (cpu/cuda) donde se ejecutan los cálculos.\n",
    "        bos_token_str (str): Token de inicio de secuencia.\n",
    "        eos_token_str (str): Token de fin de secuencia.\n",
    "        unk_token_str (str): Token para palabras desconocidas.\n",
    "\n",
    "    Returns:\n",
    "        str: La oración traducida.\n",
    "        None: Si ocurren errores críticos (ej. vocabularios no disponibles).\n",
    "    \"\"\"\n",
    "\n",
    "    if src_vocab is None: src_vocab = vocab_transform.get(SRC_LANGUAGE)\n",
    "    if trg_vocab is None: trg_vocab = vocab_transform.get(TRG_LANGUAGE)\n",
    "    if src_tokenizer is None: src_tokenizer = token_transform.get(SRC_LANGUAGE)\n",
    "    if device_to_use is None: device_to_use = DEVICE\n",
    "\n",
    "\n",
    "    if not all([src_vocab, trg_vocab, src_tokenizer, device_to_use, model_to_decode]):\n",
    "        print(\"Error en greedy_search_decode: Faltan componentes esenciales (modelo, vocabularios, tokenizador o dispositivo).\")\n",
    "        return None\n",
    "\n",
    "    model_to_decode.eval()\n",
    "\n",
    "    src_tensor = preprocess_sentence(\n",
    "        src_sentence_str,\n",
    "        src_vocab,\n",
    "        src_tokenizer,\n",
    "        device_to_use,\n",
    "        bos_token=bos_token_str,\n",
    "        eos_token=eos_token_str,\n",
    "        unk_token=unk_token_str\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model_to_decode.encoder(src_tensor)\n",
    "\n",
    "    try:\n",
    "        trg_bos_idx = trg_vocab[bos_token_str]\n",
    "    except KeyError:\n",
    "        print(f\"Error: El token '{bos_token_str}' no se encuentra en el vocabulario objetivo.\")\n",
    "        return None\n",
    "        \n",
    "    trg_indexes = [trg_bos_idx]\n",
    "    try:\n",
    "        trg_eos_idx = trg_vocab[eos_token_str]\n",
    "    except KeyError:\n",
    "        print(f\"Error: El token '{eos_token_str}' no se encuentra en el vocabulario objetivo.\")\n",
    "        trg_eos_idx = -1 # Un valor que probablemente no coincidirá\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        trg_token_tensor = torch.LongTensor([trg_indexes[-1]]).to(device_to_use)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_logits, hidden, cell = model_to_decode.decoder(trg_token_tensor, hidden, cell)\n",
    "        \n",
    "        predicted_token_index = output_logits.argmax(1).item()\n",
    "        trg_indexes.append(predicted_token_index)\n",
    "\n",
    "        if predicted_token_index == trg_eos_idx:\n",
    "            break\n",
    "            \n",
    "\n",
    "    itos_map = trg_vocab.get_itos() \n",
    "    trg_tokens = [itos_map[i] for i in trg_indexes]\n",
    "\n",
    "    if trg_tokens and trg_tokens[0] == bos_token_str:\n",
    "        trg_tokens = trg_tokens[1:]\n",
    "    if trg_tokens and trg_tokens[-1] == eos_token_str:\n",
    "        trg_tokens = trg_tokens[:-1]\n",
    "        \n",
    "    return \" \".join(trg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Seq2Seq, Dispositivo: cpu\n",
      "Vocabulario Fuente (de): 19214 tokens\n",
      "Vocabulario Objetivo (en): 10837 tokens\n",
      "--------------------------------------------------\n",
      "\n",
      "Prueba 1:\n",
      "  Oración Fuente (Alemán): Ein Mann mit einem roten Helm, der auf einem Pferd reitet.\n",
      "  Traducción (Inglés)  : A a a a a a a a a a a a a a a .\n",
      "------------------------------\n",
      "\n",
      "Prueba 2:\n",
      "  Oración Fuente (Alemán): Eine Gruppe von Menschen steht vor einem Gebäude.\n",
      "  Traducción (Inglés)  : A is standing in a of a a of a a of a large . .\n",
      "------------------------------\n",
      "\n",
      "Prueba 3:\n",
      "  Oración Fuente (Alemán): Ein kleiner Junge spielt mit einem Ball.\n",
      "  Traducción (Inglés)  : This young boy playing a a game of a small small . .\n",
      "------------------------------\n",
      "\n",
      "Prueba 4:\n",
      "  Oración Fuente (Alemán): Zwei Hunde spielen im Gras.\n",
      "  Traducción (Inglés)  : A soccer players are playing a game in a game of a game .\n",
      "------------------------------\n",
      "\n",
      "Prueba 5:\n",
      "  Oración Fuente (Alemán): Die Katze sitzt auf dem Stuhl.\n",
      "  Traducción (Inglés)  : A is sitting on a bench near a large . .\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "german_sentences_to_translate = [\n",
    "    \"Ein Mann mit einem roten Helm, der auf einem Pferd reitet.\",\n",
    "    \"Eine Gruppe von Menschen steht vor einem Gebäude.\",\n",
    "    \"Ein kleiner Junge spielt mit einem Ball.\",\n",
    "    \"Zwei Hunde spielen im Gras.\",\n",
    "    \"Die Katze sitzt auf dem Stuhl.\"\n",
    "]\n",
    "\n",
    "model_ready = 'model' in locals() and isinstance(model, Seq2Seq) and hasattr(model, 'encoder')\n",
    "vocabs_ready = (\n",
    "    SRC_LANGUAGE in vocab_transform and TRG_LANGUAGE in vocab_transform and\n",
    "    len(vocab_transform.get(SRC_LANGUAGE, [])) > 4 and \n",
    "    len(vocab_transform.get(TRG_LANGUAGE, [])) > 4  \n",
    ")\n",
    "tokenizer_ready = SRC_LANGUAGE in token_transform\n",
    "\n",
    "print(f\"Modelo: {type(model).__name__}, Dispositivo: {DEVICE}\")\n",
    "print(f\"Vocabulario Fuente ({SRC_LANGUAGE}): {len(vocab_transform[SRC_LANGUAGE])} tokens\")\n",
    "print(f\"Vocabulario Objetivo ({TRG_LANGUAGE}): {len(vocab_transform[TRG_LANGUAGE])} tokens\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, german_sentence in enumerate(german_sentences_to_translate):\n",
    "    print(f\"\\nPrueba {i+1}:\")\n",
    "    print(f\"  Oración Fuente (Alemán): {german_sentence}\")\n",
    "    \n",
    "    english_translation = greedy_search_decode(\n",
    "        model_to_decode=model,\n",
    "        src_sentence_str=german_sentence,\n",
    "        max_len=50 \n",
    "    )\n",
    "    \n",
    "    if english_translation is not None:\n",
    "        print(f\"  Traducción (Inglés)  : {english_translation}\")\n",
    "    else:\n",
    "        print(\"  Error: La decodificación no pudo generar una traducción.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def greedy_search_decode_verbose(model_to_decode, src_sentence_str: str, max_len: int = 50,\n",
    "                                 src_vocab=None, trg_vocab=None, src_tokenizer=None, device_to_use=None,\n",
    "                                 bos_token_str='<bos>', eos_token_str='<eos>', unk_token_str='<unk>',\n",
    "                                 top_n_predictions_to_show=5): \n",
    "    \"\"\"\n",
    "    Genera una traducción usando Greedy Search, mostrando el proceso paso a paso.\n",
    "    \"\"\"\n",
    "\n",
    "    if src_vocab is None: src_vocab = vocab_transform.get(SRC_LANGUAGE)\n",
    "    if trg_vocab is None: trg_vocab = vocab_transform.get(TRG_LANGUAGE)\n",
    "    if src_tokenizer is None: src_tokenizer = token_transform.get(SRC_LANGUAGE)\n",
    "    if device_to_use is None: device_to_use = DEVICE\n",
    "\n",
    "    if not all([src_vocab, trg_vocab, src_tokenizer, device_to_use, model_to_decode]):\n",
    "        print(\"Error en greedy_search_decode_verbose: Faltan componentes esenciales.\")\n",
    "        return None\n",
    "\n",
    "    model_to_decode.eval()\n",
    "    print(f\"\\n--- Decodificando (Greedy Search Detallado) para: '{src_sentence_str}' ---\")\n",
    "\n",
    "    src_tensor = preprocess_sentence(\n",
    "        src_sentence_str, src_vocab, src_tokenizer, device_to_use,\n",
    "        bos_token=bos_token_str, eos_token=eos_token_str, unk_token=unk_token_str\n",
    "    )\n",
    "    if src_tensor is None: return None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model_to_decode.encoder(src_tensor)\n",
    "\n",
    "    try:\n",
    "        trg_bos_idx = trg_vocab[bos_token_str]\n",
    "        trg_eos_idx = trg_vocab[eos_token_str]\n",
    "        itos_map_trg = trg_vocab.get_itos() \n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Token especial '{e}' no encontrado en el vocabulario objetivo.\")\n",
    "        return None\n",
    "        \n",
    "    trg_indexes = [trg_bos_idx]\n",
    "    decoded_words = [] \n",
    "\n",
    "    print(f\"Paso 0: Token inicial de entrada al decodificador: {bos_token_str} (Índice: {trg_bos_idx})\")\n",
    "\n",
    "    for i in range(max_len):\n",
    "        current_input_token_idx = trg_indexes[-1]\n",
    "        current_input_token_str = itos_map_trg[current_input_token_idx]\n",
    "        \n",
    "        trg_token_tensor = torch.LongTensor([current_input_token_idx]).to(device_to_use)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_logits, hidden, cell = model_to_decode.decoder(trg_token_tensor, hidden, cell)\n",
    "        \n",
    "        output_probs = F.softmax(output_logits, dim=1)\n",
    "        \n",
    "        predicted_token_index = output_logits.argmax(1).item()\n",
    "        predicted_token_str = itos_map_trg[predicted_token_index]\n",
    "        predicted_token_prob = output_probs[0, predicted_token_index].item()\n",
    "\n",
    "        print(f\"\\n  Paso de Decodificación {i+1}:\")\n",
    "        print(f\"    Entrada al decodificador: '{current_input_token_str}' (Índice: {current_input_token_idx})\")\n",
    "        \n",
    "        # Mostrar las N predicciones más probables\n",
    "        top_k_probs, top_k_indices = torch.topk(output_probs, top_n_predictions_to_show, dim=1)\n",
    "        print(f\"    Predicciones del decodificador (Top {top_n_predictions_to_show}):\")\n",
    "        for k in range(top_n_predictions_to_show):\n",
    "            token_idx_k = top_k_indices[0, k].item()\n",
    "            token_str_k = itos_map_trg[token_idx_k]\n",
    "            token_prob_k = top_k_probs[0, k].item()\n",
    "            print(f\"      - '{token_str_k}' (Índice: {token_idx_k}, Prob: {token_prob_k:.4f})\")\n",
    "        \n",
    "        print(f\"    Token SELECCIONADO (Greedy): '{predicted_token_str}' (Índice: {predicted_token_index}, Prob: {predicted_token_prob:.4f})\")\n",
    "\n",
    "        trg_indexes.append(predicted_token_index)\n",
    "        \n",
    "        if predicted_token_index == trg_eos_idx:\n",
    "            print(f\"    Token de Fin de Secuencia ({eos_token_str}) detectado. Finalizando decodificación.\")\n",
    "            break\n",
    "\n",
    "    final_tokens = []\n",
    "    for idx in trg_indexes:\n",
    "        if idx == trg_bos_idx:\n",
    "            continue # Saltar BOS\n",
    "        if idx == trg_eos_idx:\n",
    "            break # Detenerse en EOS\n",
    "        final_tokens.append(itos_map_trg[idx])\n",
    "        \n",
    "    final_translation = \" \".join(final_tokens)\n",
    "    return final_translation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: Seq2Seq, Dispositivo: cpu\n",
      "Vocabulario Fuente (de): 19214 tokens\n",
      "Vocabulario Objetivo (en): 10837 tokens\n",
      "--------------------------------------------------\n",
      "\n",
      "Prueba 1:\n",
      "  Oración Fuente (Alemán): Zwei Hunde spielen im Gras.\n",
      "\n",
      "--- Decodificando (Greedy Search Detallado) para: 'Zwei Hunde spielen im Gras.' ---\n",
      "Paso 0: Token inicial de entrada al decodificador: <bos> (Índice: 2)\n",
      "\n",
      "  Paso de Decodificación 1:\n",
      "    Entrada al decodificador: '<bos>' (Índice: 2)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'A' (Índice: 6, Prob: 0.2273)\n",
      "      - 'Three' (Índice: 59, Prob: 0.1925)\n",
      "      - 'There' (Índice: 223, Prob: 0.1497)\n",
      "      - 'Many' (Índice: 381, Prob: 0.0898)\n",
      "      - 'Several' (Índice: 165, Prob: 0.0795)\n",
      "    Token SELECCIONADO (Greedy): 'A' (Índice: 6, Prob: 0.2273)\n",
      "\n",
      "  Paso de Decodificación 2:\n",
      "    Entrada al decodificador: 'A' (Índice: 6)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'soccer' (Índice: 129, Prob: 0.2084)\n",
      "      - 'hockey' (Índice: 377, Prob: 0.0626)\n",
      "      - 'group' (Índice: 39, Prob: 0.0508)\n",
      "      - 'three' (Índice: 222, Prob: 0.0318)\n",
      "      - 'football' (Índice: 202, Prob: 0.0282)\n",
      "    Token SELECCIONADO (Greedy): 'soccer' (Índice: 129, Prob: 0.2084)\n",
      "\n",
      "  Paso de Decodificación 3:\n",
      "    Entrada al decodificador: 'soccer' (Índice: 129)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'players' (Índice: 221, Prob: 0.5772)\n",
      "      - 'team' (Índice: 244, Prob: 0.0615)\n",
      "      - 'members' (Índice: 935, Prob: 0.0313)\n",
      "      - 'game' (Índice: 137, Prob: 0.0287)\n",
      "      - 'playing' (Índice: 38, Prob: 0.0275)\n",
      "    Token SELECCIONADO (Greedy): 'players' (Índice: 221, Prob: 0.5772)\n",
      "\n",
      "  Paso de Decodificación 4:\n",
      "    Entrada al decodificador: 'players' (Índice: 221)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'are' (Índice: 17, Prob: 0.2928)\n",
      "      - 'playing' (Índice: 38, Prob: 0.2393)\n",
      "      - 'play' (Índice: 127, Prob: 0.1055)\n",
      "      - 'and' (Índice: 11, Prob: 0.0273)\n",
      "      - 'players' (Índice: 221, Prob: 0.0243)\n",
      "    Token SELECCIONADO (Greedy): 'are' (Índice: 17, Prob: 0.2928)\n",
      "\n",
      "  Paso de Decodificación 5:\n",
      "    Entrada al decodificador: 'are' (Índice: 17)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'playing' (Índice: 38, Prob: 0.5512)\n",
      "      - 'a' (Índice: 4, Prob: 0.0585)\n",
      "      - 'in' (Índice: 7, Prob: 0.0453)\n",
      "      - 'on' (Índice: 9, Prob: 0.0229)\n",
      "      - 'play' (Índice: 127, Prob: 0.0170)\n",
      "    Token SELECCIONADO (Greedy): 'playing' (Índice: 38, Prob: 0.5512)\n",
      "\n",
      "  Paso de Decodificación 6:\n",
      "    Entrada al decodificador: 'playing' (Índice: 38)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'a' (Índice: 4, Prob: 0.2155)\n",
      "      - 'in' (Índice: 7, Prob: 0.1399)\n",
      "      - 'the' (Índice: 8, Prob: 0.0986)\n",
      "      - 'on' (Índice: 9, Prob: 0.0626)\n",
      "      - 'game' (Índice: 137, Prob: 0.0379)\n",
      "    Token SELECCIONADO (Greedy): 'a' (Índice: 4, Prob: 0.2155)\n",
      "\n",
      "  Paso de Decodificación 7:\n",
      "    Entrada al decodificador: 'a' (Índice: 4)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'game' (Índice: 137, Prob: 0.2746)\n",
      "      - 'a' (Índice: 4, Prob: 0.1299)\n",
      "      - 'the' (Índice: 8, Prob: 0.0901)\n",
      "      - 'on' (Índice: 9, Prob: 0.0247)\n",
      "      - 'in' (Índice: 7, Prob: 0.0235)\n",
      "    Token SELECCIONADO (Greedy): 'game' (Índice: 137, Prob: 0.2746)\n",
      "\n",
      "  Paso de Decodificación 8:\n",
      "    Entrada al decodificador: 'game' (Índice: 137)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'in' (Índice: 7, Prob: 0.1269)\n",
      "      - 'of' (Índice: 13, Prob: 0.1059)\n",
      "      - ',' (Índice: 15, Prob: 0.0894)\n",
      "      - 'on' (Índice: 9, Prob: 0.0662)\n",
      "      - 'game' (Índice: 137, Prob: 0.0562)\n",
      "    Token SELECCIONADO (Greedy): 'in' (Índice: 7, Prob: 0.1269)\n",
      "\n",
      "  Paso de Decodificación 9:\n",
      "    Entrada al decodificador: 'in' (Índice: 7)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'a' (Índice: 4, Prob: 0.4390)\n",
      "      - 'the' (Índice: 8, Prob: 0.1777)\n",
      "      - 'an' (Índice: 28, Prob: 0.0286)\n",
      "      - 'game' (Índice: 137, Prob: 0.0179)\n",
      "      - 'in' (Índice: 7, Prob: 0.0174)\n",
      "    Token SELECCIONADO (Greedy): 'a' (Índice: 4, Prob: 0.4390)\n",
      "\n",
      "  Paso de Decodificación 10:\n",
      "    Entrada al decodificador: 'a' (Índice: 4)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'game' (Índice: 137, Prob: 0.1147)\n",
      "      - 'field' (Índice: 90, Prob: 0.0363)\n",
      "      - 'race' (Índice: 172, Prob: 0.0257)\n",
      "      - 'classroom' (Índice: 654, Prob: 0.0234)\n",
      "      - 'stadium' (Índice: 775, Prob: 0.0217)\n",
      "    Token SELECCIONADO (Greedy): 'game' (Índice: 137, Prob: 0.1147)\n",
      "\n",
      "  Paso de Decodificación 11:\n",
      "    Entrada al decodificador: 'game' (Índice: 137)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'of' (Índice: 13, Prob: 0.1927)\n",
      "      - '.' (Índice: 5, Prob: 0.1511)\n",
      "      - 'game' (Índice: 137, Prob: 0.0646)\n",
      "      - ',' (Índice: 15, Prob: 0.0514)\n",
      "      - 'in' (Índice: 7, Prob: 0.0463)\n",
      "    Token SELECCIONADO (Greedy): 'of' (Índice: 13, Prob: 0.1927)\n",
      "\n",
      "  Paso de Decodificación 12:\n",
      "    Entrada al decodificador: 'of' (Índice: 13)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'a' (Índice: 4, Prob: 0.2615)\n",
      "      - 'the' (Índice: 8, Prob: 0.1313)\n",
      "      - 'their' (Índice: 70, Prob: 0.0420)\n",
      "      - 'them' (Índice: 159, Prob: 0.0395)\n",
      "      - '<eos>' (Índice: 3, Prob: 0.0211)\n",
      "    Token SELECCIONADO (Greedy): 'a' (Índice: 4, Prob: 0.2615)\n",
      "\n",
      "  Paso de Decodificación 13:\n",
      "    Entrada al decodificador: 'a' (Índice: 4)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'game' (Índice: 137, Prob: 0.0669)\n",
      "      - '.' (Índice: 5, Prob: 0.0507)\n",
      "      - '<eos>' (Índice: 3, Prob: 0.0137)\n",
      "      - 'race' (Índice: 172, Prob: 0.0118)\n",
      "      - 'of' (Índice: 13, Prob: 0.0115)\n",
      "    Token SELECCIONADO (Greedy): 'game' (Índice: 137, Prob: 0.0669)\n",
      "\n",
      "  Paso de Decodificación 14:\n",
      "    Entrada al decodificador: 'game' (Índice: 137)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - '.' (Índice: 5, Prob: 0.3010)\n",
      "      - 'of' (Índice: 13, Prob: 0.2404)\n",
      "      - 'on' (Índice: 9, Prob: 0.0311)\n",
      "      - ',' (Índice: 15, Prob: 0.0256)\n",
      "      - 'in' (Índice: 7, Prob: 0.0249)\n",
      "    Token SELECCIONADO (Greedy): '.' (Índice: 5, Prob: 0.3010)\n",
      "\n",
      "  Paso de Decodificación 15:\n",
      "    Entrada al decodificador: '.' (Índice: 5)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - '<eos>' (Índice: 3, Prob: 0.5719)\n",
      "      - '.' (Índice: 5, Prob: 0.0466)\n",
      "      - 'the' (Índice: 8, Prob: 0.0421)\n",
      "      - 'a' (Índice: 4, Prob: 0.0174)\n",
      "      - 'their' (Índice: 70, Prob: 0.0157)\n",
      "    Token SELECCIONADO (Greedy): '<eos>' (Índice: 3, Prob: 0.5719)\n",
      "    Token de Fin de Secuencia (<eos>) detectado. Finalizando decodificación.\n",
      "  Traducción (Inglés)  : A soccer players are playing a game in a game of a game .\n",
      "------------------------------\n",
      "\n",
      "Prueba 2:\n",
      "  Oración Fuente (Alemán): Die Katze sitzt auf dem Stuhl.\n",
      "\n",
      "--- Decodificando (Greedy Search Detallado) para: 'Die Katze sitzt auf dem Stuhl.' ---\n",
      "Paso 0: Token inicial de entrada al decodificador: <bos> (Índice: 2)\n",
      "\n",
      "  Paso de Decodificación 1:\n",
      "    Entrada al decodificador: '<bos>' (Índice: 2)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'A' (Índice: 6, Prob: 0.1262)\n",
      "      - 'In' (Índice: 804, Prob: 0.0824)\n",
      "      - 'Spectators' (Índice: 2012, Prob: 0.0482)\n",
      "      - 'There' (Índice: 223, Prob: 0.0416)\n",
      "      - 'At' (Índice: 1330, Prob: 0.0401)\n",
      "    Token SELECCIONADO (Greedy): 'A' (Índice: 6, Prob: 0.1262)\n",
      "\n",
      "  Paso de Decodificación 2:\n",
      "    Entrada al decodificador: 'A' (Índice: 6)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'is' (Índice: 10, Prob: 0.1200)\n",
      "      - 'are' (Índice: 17, Prob: 0.1056)\n",
      "      - 'young' (Índice: 25, Prob: 0.0597)\n",
      "      - 'sitting' (Índice: 32, Prob: 0.0309)\n",
      "      - 'the' (Índice: 8, Prob: 0.0267)\n",
      "    Token SELECCIONADO (Greedy): 'is' (Índice: 10, Prob: 0.1200)\n",
      "\n",
      "  Paso de Decodificación 3:\n",
      "    Entrada al decodificador: 'is' (Índice: 10)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'sitting' (Índice: 32, Prob: 0.1175)\n",
      "      - 'a' (Índice: 4, Prob: 0.1147)\n",
      "      - 'on' (Índice: 9, Prob: 0.0869)\n",
      "      - 'the' (Índice: 8, Prob: 0.0548)\n",
      "      - 'at' (Índice: 20, Prob: 0.0504)\n",
      "    Token SELECCIONADO (Greedy): 'sitting' (Índice: 32, Prob: 0.1175)\n",
      "\n",
      "  Paso de Decodificación 4:\n",
      "    Entrada al decodificador: 'sitting' (Índice: 32)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'on' (Índice: 9, Prob: 0.4475)\n",
      "      - 'at' (Índice: 20, Prob: 0.1512)\n",
      "      - 'a' (Índice: 4, Prob: 0.1020)\n",
      "      - 'in' (Índice: 7, Prob: 0.0561)\n",
      "      - 'the' (Índice: 8, Prob: 0.0387)\n",
      "    Token SELECCIONADO (Greedy): 'on' (Índice: 9, Prob: 0.4475)\n",
      "\n",
      "  Paso de Decodificación 5:\n",
      "    Entrada al decodificador: 'on' (Índice: 9)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'a' (Índice: 4, Prob: 0.4904)\n",
      "      - 'the' (Índice: 8, Prob: 0.2691)\n",
      "      - 'on' (Índice: 9, Prob: 0.0542)\n",
      "      - 'at' (Índice: 20, Prob: 0.0151)\n",
      "      - 'in' (Índice: 7, Prob: 0.0110)\n",
      "    Token SELECCIONADO (Greedy): 'a' (Índice: 4, Prob: 0.4904)\n",
      "\n",
      "  Paso de Decodificación 6:\n",
      "    Entrada al decodificador: 'a' (Índice: 4)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'bench' (Índice: 148, Prob: 0.0370)\n",
      "      - 'a' (Índice: 4, Prob: 0.0322)\n",
      "      - 'the' (Índice: 8, Prob: 0.0247)\n",
      "      - 'sidewalk' (Índice: 89, Prob: 0.0197)\n",
      "      - 'dock' (Índice: 459, Prob: 0.0188)\n",
      "    Token SELECCIONADO (Greedy): 'bench' (Índice: 148, Prob: 0.0370)\n",
      "\n",
      "  Paso de Decodificación 7:\n",
      "    Entrada al decodificador: 'bench' (Índice: 148)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'near' (Índice: 84, Prob: 0.0710)\n",
      "      - 'looking' (Índice: 56, Prob: 0.0542)\n",
      "      - 'of' (Índice: 13, Prob: 0.0436)\n",
      "      - 'on' (Índice: 9, Prob: 0.0294)\n",
      "      - ',' (Índice: 15, Prob: 0.0254)\n",
      "    Token SELECCIONADO (Greedy): 'near' (Índice: 84, Prob: 0.0710)\n",
      "\n",
      "  Paso de Decodificación 8:\n",
      "    Entrada al decodificador: 'near' (Índice: 84)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'a' (Índice: 4, Prob: 0.3735)\n",
      "      - 'the' (Índice: 8, Prob: 0.2810)\n",
      "      - 'an' (Índice: 28, Prob: 0.0210)\n",
      "      - 'their' (Índice: 70, Prob: 0.0190)\n",
      "      - 'to' (Índice: 18, Prob: 0.0165)\n",
      "    Token SELECCIONADO (Greedy): 'a' (Índice: 4, Prob: 0.3735)\n",
      "\n",
      "  Paso de Decodificación 9:\n",
      "    Entrada al decodificador: 'a' (Índice: 4)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - 'large' (Índice: 62, Prob: 0.0307)\n",
      "      - 'camera' (Índice: 118, Prob: 0.0136)\n",
      "      - 'car' (Índice: 143, Prob: 0.0127)\n",
      "      - 'sign' (Índice: 207, Prob: 0.0094)\n",
      "      - 'the' (Índice: 8, Prob: 0.0093)\n",
      "    Token SELECCIONADO (Greedy): 'large' (Índice: 62, Prob: 0.0307)\n",
      "\n",
      "  Paso de Decodificación 10:\n",
      "    Entrada al decodificador: 'large' (Índice: 62)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - '.' (Índice: 5, Prob: 0.0394)\n",
      "      - 'sign' (Índice: 207, Prob: 0.0200)\n",
      "      - 'day' (Índice: 190, Prob: 0.0180)\n",
      "      - 'of' (Índice: 13, Prob: 0.0138)\n",
      "      - 'event' (Índice: 364, Prob: 0.0132)\n",
      "    Token SELECCIONADO (Greedy): '.' (Índice: 5, Prob: 0.0394)\n",
      "\n",
      "  Paso de Decodificación 11:\n",
      "    Entrada al decodificador: '.' (Índice: 5)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - '.' (Índice: 5, Prob: 0.1402)\n",
      "      - '<eos>' (Índice: 3, Prob: 0.0843)\n",
      "      - 'the' (Índice: 8, Prob: 0.0299)\n",
      "      - 'of' (Índice: 13, Prob: 0.0249)\n",
      "      - 'day' (Índice: 190, Prob: 0.0147)\n",
      "    Token SELECCIONADO (Greedy): '.' (Índice: 5, Prob: 0.1402)\n",
      "\n",
      "  Paso de Decodificación 12:\n",
      "    Entrada al decodificador: '.' (Índice: 5)\n",
      "    Predicciones del decodificador (Top 5):\n",
      "      - '<eos>' (Índice: 3, Prob: 0.3907)\n",
      "      - '.' (Índice: 5, Prob: 0.0934)\n",
      "      - 'the' (Índice: 8, Prob: 0.0575)\n",
      "      - 'of' (Índice: 13, Prob: 0.0168)\n",
      "      - 'a' (Índice: 4, Prob: 0.0138)\n",
      "    Token SELECCIONADO (Greedy): '<eos>' (Índice: 3, Prob: 0.3907)\n",
      "    Token de Fin de Secuencia (<eos>) detectado. Finalizando decodificación.\n",
      "  Traducción (Inglés)  : A is sitting on a bench near a large . .\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "german_sentences_to_translate = [\n",
    "    \"Zwei Hunde spielen im Gras.\",\n",
    "    \"Die Katze sitzt auf dem Stuhl.\"\n",
    "]\n",
    "\n",
    "model_ready = 'model' in locals() and isinstance(model, Seq2Seq) and hasattr(model, 'encoder')\n",
    "vocabs_ready = (\n",
    "    SRC_LANGUAGE in vocab_transform and TRG_LANGUAGE in vocab_transform and\n",
    "    len(vocab_transform.get(SRC_LANGUAGE, [])) > 4 and \n",
    "    len(vocab_transform.get(TRG_LANGUAGE, [])) > 4  \n",
    ")\n",
    "tokenizer_ready = SRC_LANGUAGE in token_transform\n",
    "\n",
    "\n",
    "print(f\"Modelo: {type(model).__name__}, Dispositivo: {DEVICE}\")\n",
    "print(f\"Vocabulario Fuente ({SRC_LANGUAGE}): {len(vocab_transform[SRC_LANGUAGE])} tokens\")\n",
    "print(f\"Vocabulario Objetivo ({TRG_LANGUAGE}): {len(vocab_transform[TRG_LANGUAGE])} tokens\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, german_sentence in enumerate(german_sentences_to_translate):\n",
    "    print(f\"\\nPrueba {i+1}:\")\n",
    "    print(f\"  Oración Fuente (Alemán): {german_sentence}\")\n",
    "    \n",
    "\n",
    "    english_translation = greedy_search_decode_verbose(\n",
    "        model_to_decode=model,\n",
    "        src_sentence_str=german_sentence,\n",
    "        max_len=50 \n",
    "    )\n",
    "    \n",
    "    if english_translation is not None:\n",
    "        print(f\"  Traducción (Inglés)  : {english_translation}\")\n",
    "    else:\n",
    "        print(\"  Error: La decodificación no pudo generar una traducción.\")\n",
    "    print(\"-\" * 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
